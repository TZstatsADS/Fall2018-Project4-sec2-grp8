---
title: "OCR Post-Processing"
author: "Sec2 Group8"
output:
  html_document:
    df_print: paged
---

GU4243/GR5243: Applied Data Science

<style type="text/css">
h1.title {
  font-size: 24px;
  color: Black;
}
h1 { /* Header 1 */
  font-size: 24px;
  color: Black;
}
h2 { /* Header 2 */
  font-size: 20px;
  color: Black;
}
h3 { /* Header 3 */
  font-size: 16px;
  color: Black;
}
h4 { /* Header 4 */
  font-size: 14px;
  color: Grey;
}
</style>

# Introduction {-}

Optical character recognition (OCR) is the process of converting scanned images of machine printed or
handwritten text (numerals, letters, and symbols), into machine readable character streams, plain (e.g. text files) or formatted (e.g. HTML files). As shown in Figure 1, the data *workflow* in a typical OCR system consists of three major stages:

* Pre-processing

* OCR character recognition

* Post-processing

![](../figs/ocr_flowchart.png) 

In this project, we are going to **focus on the third stage -- post-processing**, which includes two tasks: *error detection* and *error correction*.  

# Step 1 - Load library and source code

```{r load library, warning=FALSE, message = FALSE}
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
  ## devtools is required
  library(devtools)
  install_github("trinker/pacman")
}
if(!require("ebmc")) install.packages("ebmc")
if(!require("dplyr")) install.packages("dplyr")
library(ebmc)
library(caret)
library(dplyr)

pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
```


# Step 2 - read the files and conduct Tesseract OCR

Although we have processed the Tesseract OCR and save the output txt files in the `data` folder, we include this chunk of code in order to make clear the whole pipeline.

```{r read file, eval=FALSE}
# for(i in c(1:length(file_name_vec))){
#   current_file_name <- sub(".txt","",file_name_vec[i])
#   ## png folder is not provided on github (the code is only on demonstration purpose)
#   current_tesseract_txt <- tesseract::ocr(paste("../data/png/",current_file_name,".png",sep=""))
#   
#   ### clean the tessetact text (separate line by "\n", delete null string, transter to lower case)
#   clean_tesseract_txt <- strsplit(current_tesseract_txt,"\n")[[1]]
#   clean_tesseract_txt <- clean_tesseract_txt[clean_tesseract_txt!=""]
#   
#   ### save tesseract text file
#   writeLines(clean_tesseract_txt, paste("../data/tesseract/",current_file_name,".txt",sep=""))
# }
```


# Step 3 - Data preprocessing

Because the word from ground truth and tesseract are not perfectly one on one matched, so in this step we preprocess our data so in the following steps we can work on error detection and correction by comparing each word from Tesseract and Ground Truth.

## Find pairs of Tesseract and ground truth files do not have the same number of lines. 

```{r preprocessing1}
# read ground truth text into 1 list
n_file = length(file_name_vec)
tesseract_vec_all = list() # each element is a vector of words for each text file
tesseract_lines_all = list() # each element is a list of lines for each text file
gt_vec_all = list() # each element is a vector of words for each text file
gt_lines_all = list() # each element is a list of lines for each text file
len_check = rep(NA, 100)
len_check2 = rep(NA, 100)

## check if each tesseract file has the same number of length with its corresponding ground truth

for(i in 1:n_file){
  ## i represents that this is the i-th file we are dealing with
  current_file_name <- sub(".txt","",file_name_vec[i])
  
  ## read the ground truth text, save it by lines & vectorize it
  current_ground_truth_txt = readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
  
  ## read the tesseract text, save it by lines & vectorize it
  current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)
  
  ## check if each tesseract file has the same number of length with its corresponding ground truth
  len_check[i] = length(current_ground_truth_txt) == length(current_tesseract_txt)
  ## check if all ground truth are not shorter than its Tesseract
  len_check2[i] = length(current_ground_truth_txt) >= length(current_tesseract_txt)
}

# there exist pairs that do not have same number of lines:
which(!len_check) 
# all ground truth files have at least same number of lines compared with corresponding Tesseract files:
which(!len_check2) 
n_file-sum(len_check)
```

The above chunck shows that 13 pairs of Tesseract and ground truth files do not have the same number of lines, and for such pairs, the ground truth files always have more lines than the Tesseract files.

Therefore, we trimmed the ground truth files manually until they have the same length with the corresponding Tesseract files.

To simplify the word alignment process, we only work on the line pairs that have same number of words. As calculated below, only 11% of lines are discarded this way, our method can still work for the major cases.

All the process stated below are based on trimmed files, same length line pairs.

## Delete rows in ground truth and tesseract which character number donâ€™t match.

```{r preprocessing2}
same_na = rep(NA, n_file)

## count number of lines in total
raw_lines = 0
processed_lines = 0
word_num_gt = rep(NA, n_file)
word_num_tes = rep(NA, n_file)

for(i in 1:n_file){
  ## i represents that this is the i-th file we are dealing with
  current_file_name <- sub(".txt","",file_name_vec[i])
  
  ## read the ground truth text
  current_ground_truth_txt = readLines(paste("../data/ground_truth_trimmed/",current_file_name,".txt",sep=""), warn=FALSE)
  
  ## read the tesseract text
  current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)

  ## for evaluation purpose
  ## count the total number of words in each ground truth and Tesseract file
  word_num_gt[i] = length(str_split(paste(current_ground_truth_txt, collapse = " ")," ")[[1]])
  word_num_tes[i] = length(str_split(paste(current_tesseract_txt, collapse = " ")," ")[[1]])

  ## only keep the line pairs that have the same number of words
  n_line = length(current_ground_truth_txt)
  raw_lines = n_line + raw_lines
  
  for (j in 1:n_line){
    same_len_logi = length(str_split(current_ground_truth_txt[j], " ")[[1]]) == 
      length(str_split(current_tesseract_txt[j], " ")[[1]])
    if(!same_len_logi){
      current_ground_truth_txt[j] = NA
      current_tesseract_txt[j] = NA
    }
  }
  na_logi1 = is.na(current_ground_truth_txt)
  na_logi2 = is.na(current_tesseract_txt)
  same_na[i] = all(na_logi1 == na_logi2) # check if all the lines are assigned accordingly
  
  ## save and vectorized cleaned text
  gt_lines_all[[i]] = current_ground_truth_txt[!na_logi1]
  gt_vec_all[[i]] <- str_split(paste(gt_lines_all[[i]], collapse = " ")," ")[[1]]
  tesseract_lines_all[[i]] = current_tesseract_txt[!na_logi2]
  tesseract_vec_all[[i]] = str_split(paste(tesseract_lines_all[[i]], collapse = " ")," ")[[1]]
  
  processed_lines = length(gt_lines_all[[i]]) + processed_lines
  
  ## re-check if each tesseract file has the same number of length with its corresponding ground truth
  len_check[i] = length(tesseract_lines_all[[i]]) == length(gt_lines_all[[i]])
}
## recheck if lines are removed correctly
all(same_na)
## re-check if each tesseract file has the same number of length with its corresponding ground truth
all(len_check)

## recheck if all the words in a pair of files can be mapped 1 to 1 now
map11 = rep(NA,n_file)
for(i in 1:n_file){
  map11[i] = length(tesseract_vec_all[[i]]) == length(gt_vec_all[[i]])
}
all(map11)

## What is the proportion of lines deleted because of unequal number of words
1 - processed_lines/raw_lines # = 0.1132
```

After this process we delete about 11.32% of the data, and saved processed data in a new folder called "ground_truth_trimmed". 

For the following steps we proceed with this "ground_truth_trimmed" dataset.


# Step 4 - Error detection

Now, we are ready to conduct post-processing, based on the Tessearct OCR output. First of all, we need to detect errors, or *incorrectly processed words* -- check to see if an input string is a valid dictionary word or if its n-grams are all legal.

The referenced papers are:

 [Rule-based techniques](http://webpages.ursinus.edu/akontostathis/KulpKontostathisFinal.pdf)

- rules are in the section 2.2 

Each element in the list in this following chunck is a dataframe for a pair of files. Each row in a dataframe contains the Tesseract word, its ground truth word and the detect output.

The CPU time charged for the execution: 1909.355
The CPU time charged for the execution of user instructions: 484.844

```{r detection}
MatchDetect = list()
MatchDetectCharacter = list()
user_time1 = proc.time()[1]
system_time1 = proc.time()[2]
for(i in 1:n_file){
  tesseract_vec <- tesseract_vec_all[[i]]
  tesseract_if_clean <- unlist(lapply(tesseract_vec,ifCleanToken)) # source code of ifCleanToken in in lib folder
  mat = cbind(tesseract_vec_all[[i]], gt_vec_all[[i]],tesseract_if_clean)
  save(mat, file=paste0("../output/mats/mat_file",i,".RData"))
  df = data.frame(mat)
  df2 = data.frame(mat, stringsAsFactors = F)
  colnames(df) = c("Tesseract", "GroundTruth", "Detect")
  colnames(df2) = c("Tesseract", "GroundTruth", "Detect")
  MatchDetect[[i]] = df # character vectors are converted to factors
  MatchDetectCharacter[[i]] = df2 # character vectors are not converted to factors
}
user_time2 = proc.time()[1]
system_time2 = proc.time()[2]
user_dif = user_time2 - user_time1
system_dif = system_time2 - system_time1
cat("The CPU time charged for the execution of user instructions:", user_dif)
cat("The CPU time charged for the execution by the system:", system_dif)

save(MatchDetect, file="../output/MatchDetect.RData")
save(MatchDetectCharacter, file="../output/MatchDetectCharacter.RData")

```

# Step 5 - Error correction

Given the detected word error, in order to find the best correction, we need to generating the candidate corrections: a dictionary or a database of legal n-grams to locate one or more potential correction terms. Then we need invoke some lexical-similarity measure between the misspelled string and the candidates or a probabilistic estimate of the likelihood of the correction to rank order the candidates.

The referenced papers are:

Supervised model -- [correction regressor](https://arxiv.org/pdf/1611.06950.pdf)

## build ground_truth file
```{r candidate}
load("../output/MatchDetect.RData")
## ground truth string file
ground_truth <- NULL
ground_truth <- paste(unlist(gt_lines_all), collapse = " ")
ground_truth_vec <- str_split(ground_truth," ")[[1]] 
```

## build ngram frequency vocabulary
```{r ngram}
ground_truth <- iconv(ground_truth, "latin1", "ASCII", sub="") # pre_clean data
ground_truth <- iconv(ground_truth,to="utf-8-mac")
ground_truth_Corpus <- VCorpus(VectorSource(ground_truth)) # Make corpus
ground_truth_Corpus <- tm_map(ground_truth_Corpus, stripWhitespace) # Remove unneccesary white spaces
ground_truth_Corpus <- tm_map(ground_truth_Corpus, removePunctuation) # Remove punctuation
ground_truth_Corpus <- tm_map(ground_truth_Corpus, removeNumbers) # Remove numbers
ground_truth_Corpus <- tm_map(ground_truth_Corpus, tolower) # Convert to lowercase
ground_truth_Corpus <- tm_map(ground_truth_Corpus, PlainTextDocument) # Plain text

n_grams_vocabulary <- function(n, data) {
  options(mc.cores=1)
  # Builds n-gram tokenizer 
  tk <- function(x) NGramTokenizer(x, Weka_control(min = n, max = n))
  # Create matrix
  ngrams_matrix <- TermDocumentMatrix(data, control=list(tokenize=tk))
  # make matrix for easy view
  ngrams_matrix <- as.matrix(rollup(ngrams_matrix, 2, na.rm=TRUE, FUN=sum))
  ngrams_matrix <- data.frame(word=rownames(ngrams_matrix), freq=ngrams_matrix[,1])
  # find 20 most frequent n-grams in the matrix
  ngrams_matrix <- ngrams_matrix[order(-ngrams_matrix$freq), ]
  ngrams_matrix$word <- factor(ngrams_matrix$word, as.character(ngrams_matrix$word))
  return(ngrams_matrix)
}

#unigram <- n_grams_vocabulary(n=1, data=ground_truth_Corpus)
#three_gram <- n_grams_vocabulary(n=3, data=ground_truth_Corpus)
#save(unigram, file=paste0("../output/unigram.RData"))
#save(three_gram, file=paste0("../output/three_gram.RData"))

# plots first 20 three_gram
ggplot(three_gram[1:20,], aes(x=word, y=freq)) + 
  geom_bar(stat="Identity") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab("three-grams") + 
  ylab("Frequency")
```

## build features, create dataset
```{r features}
Sys.time()
correct_pattern <- function(single_pattern){
  return(gsub(x = single_pattern, pattern = "\\(", replacement = "\\\\\\(", ignore.case = TRUE))
}


<<<<<<< HEAD
5. [topic models](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4377099)
## build ground_truth file
```{r}
load("../output/MatchDetect.RData")
## ground truth string file
ground_truth <- NULL
ground_truth <- paste(unlist(gt_lines_all), collapse = " ")
ground_truth_vec <- str_split(ground_truth," ")[[1]] 
```

## build ngram frequency vocabulary
```{r}

ground_truth <- iconv(ground_truth, "latin1", "ASCII", sub="") # pre_clean data
ground_truth <- iconv(ground_truth,to="utf-8-mac")
ground_truth_Corpus <- VCorpus(VectorSource(ground_truth)) # Make corpus
ground_truth_Corpus <- tm_map(ground_truth_Corpus, stripWhitespace) # Remove unneccesary white spaces
ground_truth_Corpus <- tm_map(ground_truth_Corpus, removePunctuation) # Remove punctuation
ground_truth_Corpus <- tm_map(ground_truth_Corpus, removeNumbers) # Remove numbers
ground_truth_Corpus <- tm_map(ground_truth_Corpus, tolower) # Convert to lowercase
ground_truth_Corpus <- tm_map(ground_truth_Corpus, PlainTextDocument) # Plain text

n_grams_vocabulary <- function(n, data) {
  options(mc.cores=1)
  # Builds n-gram tokenizer 
  tk <- function(x) NGramTokenizer(x, Weka_control(min = n, max = n))
  # Create matrix
  ngrams_matrix <- TermDocumentMatrix(data, control=list(tokenize=tk))
  # make matrix for easy view
  ngrams_matrix <- as.matrix(rollup(ngrams_matrix, 2, na.rm=TRUE, FUN=sum))
  ngrams_matrix <- data.frame(word=rownames(ngrams_matrix), freq=ngrams_matrix[,1])
  # find 20 most frequent n-grams in the matrix
  ngrams_matrix <- ngrams_matrix[order(-ngrams_matrix$freq), ]
  ngrams_matrix$word <- factor(ngrams_matrix$word, as.character(ngrams_matrix$word))
  return(ngrams_matrix)
}

#unigram <- n_grams_vocabulary(n=1, data=ground_truth_Corpus)
#three_gram <- n_grams_vocabulary(n=3, data=ground_truth_Corpus)
#save(unigram, file=paste0("../output/unigram.RData"))
#save(three_gram, file=paste0("../output/three_gram.RData"))

# plots first 20 three_gram
ggplot(three_gram[1:20,], aes(x=word, y=freq)) + 
  geom_bar(stat="Identity") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab("three-grams") + 
  ylab("Frequency")
```

## build features, create dataset
```{r}
Sys.time()
correct_pattern <- function(single_pattern){
  return(gsub(x = single_pattern, pattern = "\\(", replacement = "\\\\\\(", ignore.case = TRUE))
}


=======
>>>>>>> d91af1b90e3970e9ac78422353bac35cd773bafb
initial_dataset <- function(){
  data_set <- data.frame("error_term" = character(0), 
                           "gt_term" = character(0), 
                           "candidate" = character(0),
                           "feature1" = integer(0),
                           "feature2" = integer(0),
                            "feature3" = integer(0),
                            "feature4" = integer(0),
                           "feature5" = integer(0),
                           "feature6" = integer(0),
                           "file_index" = integer(0),
                           "error_index" = integer(0))
  return(data_set)
  
}
<<<<<<< HEAD



## function to calcualte candidate's exact frequency
freq5 <- function(test, n_gram){
  ifelse(sum(n_gram$word == test) == 0, 0, n_gram[n_gram$word == test, ]$freq)
}

## function to calcualte candidate's relaxd frequency
freq6 <- function(test, ngram){
  test <- correct_pattern(test)
  ifelse(sum(grep(test, ngram$word)) == 0, 0, sum(ngram$freq[grep(test, ngram$word)]))
}

## function to calculate score based on context frequency
context_score <- function(frequency){
  if(max(frequency) == 0){
    return(frequency)
  }else{
    return(frequency/max(frequency))
  } 
}


# parameter of feature1
delta <- 5
# first s elements we keep in the candidate_set
s <- 3

source("../lib/string_similarity.R") # used in calculating feature2

#### initial the filepath for storing the output
filepath = "../output/"
## for each file  #length(tesseract_lines_all), c(1, 3, 4, 7, 8)
for (i in 23) {
  
  data_set = initial_dataset()
  start_time_ds <- Sys.time()
  ## create lexicon for candidate search
  # ground_truth lexicon
  current_gt <- paste(gt_lines_all[[i]], collapse = " ")
  current_gt_vec <- str_split(current_gt, " ")[[1]]
  
  additional_file <- sample((1:100)[-i],  1)
  all_gt <- paste(c(gt_lines_all[[i]], 
                    gt_lines_all[[additional_file[1]]]), collapse = " ")
  all_gt_vec <- str_split(all_gt," ")[[1]] 
  all_gt_vec <- unique(all_gt_vec)
  
  ## search error_term and corresponding ground truth for each file
  error_position <- which(MatchDetect[[i]]$Detect == FALSE)
  error_term <- as.character(MatchDetect[[i]]$Tesseract[error_position])
  gt_term <- as.character(MatchDetect[[i]]$GroundTruth[error_position])

  ## for each error  #length(error_term)
  for (j in 1:length(error_term)){
    w_e <- error_term[j] # word_error
    ## create a candidate set for each error
    leven_distance <- sapply(all_gt_vec, stringdist, b = w_e, method = "lv") # calculate Levenshtein distance between error term and each candidate in all_gt_vec
    chosen_candidate <- order(leven_distance)[1:s] # chose first s candidates position
    candidate_set <- all_gt_vec[chosen_candidate]
    
    ## feature scoring
    
    ### feature1: Levenshtein edit distance
    # Levenshtein distance for the candidate
    dist_candidate <- as.numeric(leven_distance[chosen_candidate])
    score_led <- 1 - dist_candidate/(delta + 1)
    
    ### feature2: String similarity
    score_ss <- string_similiarity(w_e, candidate_set)
    
    ### feature3: Language popularity
    frequency3 <- unlist(lapply(candidate_set, freq5, unigram))
    score_lp <- context_score(frequency3)
    
    ### feature4: lexicon existance
    score_le <- as.numeric(candidate_set%in%current_gt_vec)
    
    ### feature5 and 6: exact-context popularity andrelaxed-context popularity
    # neighboring terms
    minus2 <-  as.character(MatchDetect[[i]]$Tesseract[abs(error_position[j] - 2)])
    minus1 <- as.character(MatchDetect[[i]]$Tesseract[abs(error_position[j] - 1)])
    positive1 <-  as.character(MatchDetect[[i]]$Tesseract[min((error_position[j] + 1), nrow(MatchDetect[[i]]))])
    positive2 <-  as.character(MatchDetect[[i]]$Tesseract[min((error_position[j] + 2), nrow(MatchDetect[[i]]))])
    frequency5 <- frequency6 <- NULL
    ## feature5 and feature6
    # for each candidate, calculate ngram frequency 
    for (k in 1:length(candidate_set)) {
      # tested candidate
      candidate <- candidate_set[k]
      # feature5: exact-context popularity
      ngram1 <- paste(c(minus2, minus1, candidate), collapse = " ")
      ngram2 <- paste(c(minus1, candidate, positive1), collapse = " ")
      ngram3 <- paste(c(candidate, positive1, positive2), collapse = " ")
      # candidate ngram freq (exact)
      candidate_frequency <- sum(freq5(ngram1, three_gram), 
                                 freq5(ngram2, three_gram),   
                                 freq5(ngram3, three_gram))
      frequency5 <- append(frequency5, candidate_frequency)
      
      # feature6: relaxed-context popularity
      ngram1_1 <- paste(c(".", minus1, candidate), collapse = " ")
      ngram1_2 <- paste(c(minus2, ".", candidate), collapse = " ")

      ngram2_1 <- paste(c(".", candidate, positive1), collapse = " ")
      ngram2_2 <- paste(c(minus1, candidate, "."), collapse = " ")
      
      ngram3_1 <- paste(c(candidate, ".", positive2), collapse = " ")
      ngram3_2 <- paste(c(candidate, positive1, "."), collapse = " ")
      # candidate ngram freq (relaxed)
      candidate_frequency <- sum(freq6(ngram1_1, three_gram), 
                                 freq6(ngram1_2, three_gram), 
                                 freq6(ngram2_1, three_gram), 
                                 freq6(ngram2_2, three_gram),
                                 freq6(ngram3_1, three_gram),
                                 freq6(ngram3_2, three_gram))
      frequency6 <- append(frequency6, candidate_frequency)
    }
    
    score_ecp <- context_score(frequency5)
    score_rcp <- context_score(frequency6)

    data_set <- add_row(data_set, "error_term" = rep(w_e, length(candidate_set)), 
                           "gt_term" = rep(gt_term[j], length(candidate_set)), 
                           "candidate" = candidate_set,
                           "feature1" = score_led,
                           "feature2" = score_ss,
                          "feature4" = score_le,
                           "feature3" = score_lp,
                           "feature5" = score_ecp,
                           "feature6" = score_rcp,
                        "file_index" = rep(i, length(candidate_set)),
                        "error_index" = rep(error_position[j], length(candidate_set)))

  }
# the loop for each file
data_set$y <- as.numeric(data_set$gt_term == data_set$candidate)
end_time_ds <- Sys.time()
# time_ds <- end_time_ds - start_time_ds ## file1, 3, 4, 7, 8; first 20 errors for each file; first 5 candidates for each error
print(paste("Running time for building dataset = ", (end_time_ds - start_time_ds)))
filename = paste(paste("dataset", i, sep = "_"), "RData", sep = ".")
save(data_set, file=paste(filepath, filename))
}
# 
# data_set$y <- as.numeric(data_set$gt_term == data_set$candidate)
# end_time_ds <- Sys.time()
# time_ds <- end_time_ds - start_time_ds ## file1, 3, 4, 7, 8; first 20 errors for each file; first 5 candidates for each error
# print(paste("Running time for building dataset = ", time_ds))
# filepath = "../output/"
=======



## function to calcualte candidate's exact frequency
freq5 <- function(test, n_gram){
  ifelse(sum(n_gram$word == test) == 0, 0, n_gram[n_gram$word == test, ]$freq)
}
>>>>>>> d91af1b90e3970e9ac78422353bac35cd773bafb

## function to calcualte candidate's relaxd frequency
freq6 <- function(test, ngram){
  test <- correct_pattern(test)
  ifelse(sum(grep(test, ngram$word)) == 0, 0, sum(ngram$freq[grep(test, ngram$word)]))
}

## function to calculate score based on context frequency
context_score <- function(frequency){
  if(max(frequency) == 0){
    return(frequency)
  }else{
    return(frequency/max(frequency))
  } 
}


# parameter of feature1
delta <- 5
# first s elements we keep in the candidate_set
s <- 3

source("../lib/string_similarity.R") # used in calculating feature2

#### initial the filepath for storing the output
filepath = "../output/"
## for each file  #length(tesseract_lines_all), c(1, 3, 4, 7, 8)
for (i in 23) {
  
  data_set = initial_dataset()
  start_time_ds <- Sys.time()
  ## create lexicon for candidate search
  # ground_truth lexicon
  current_gt <- paste(gt_lines_all[[i]], collapse = " ")
  current_gt_vec <- str_split(current_gt, " ")[[1]]
  
  additional_file <- sample((1:100)[-i],  1)
  all_gt <- paste(c(gt_lines_all[[i]], 
                    gt_lines_all[[additional_file[1]]]), collapse = " ")
  all_gt_vec <- str_split(all_gt," ")[[1]] 
  all_gt_vec <- unique(all_gt_vec)
  
  ## search error_term and corresponding ground truth for each file
  error_position <- which(MatchDetect[[i]]$Detect == FALSE)
  error_term <- as.character(MatchDetect[[i]]$Tesseract[error_position])
  gt_term <- as.character(MatchDetect[[i]]$GroundTruth[error_position])

  ## for each error  #length(error_term)
  for (j in 1:length(error_term)){
    w_e <- error_term[j] # word_error
    ## create a candidate set for each error
    leven_distance <- sapply(all_gt_vec, stringdist, b = w_e, method = "lv") # calculate Levenshtein distance between error term and each candidate in all_gt_vec
    chosen_candidate <- order(leven_distance)[1:s] # chose first s candidates position
    candidate_set <- all_gt_vec[chosen_candidate]
    
    ## feature scoring
    
    ### feature1: Levenshtein edit distance
    # Levenshtein distance for the candidate
    dist_candidate <- as.numeric(leven_distance[chosen_candidate])
    score_led <- 1 - dist_candidate/(delta + 1)
    
    ### feature2: String similarity
    score_ss <- string_similiarity(w_e, candidate_set)
    
    ### feature3: Language popularity
    frequency3 <- unlist(lapply(candidate_set, freq5, unigram))
    score_lp <- context_score(frequency3)
    
    ### feature4: lexicon existance
    score_le <- as.numeric(candidate_set%in%current_gt_vec)
    
    ### feature5 and 6: exact-context popularity andrelaxed-context popularity
    # neighboring terms
    minus2 <-  as.character(MatchDetect[[i]]$Tesseract[abs(error_position[j] - 2)])
    minus1 <- as.character(MatchDetect[[i]]$Tesseract[abs(error_position[j] - 1)])
    positive1 <-  as.character(MatchDetect[[i]]$Tesseract[min((error_position[j] + 1), nrow(MatchDetect[[i]]))])
    positive2 <-  as.character(MatchDetect[[i]]$Tesseract[min((error_position[j] + 2), nrow(MatchDetect[[i]]))])
    frequency5 <- frequency6 <- NULL
    ## feature5 and feature6
    # for each candidate, calculate ngram frequency 
    for (k in 1:length(candidate_set)) {
      # tested candidate
      candidate <- candidate_set[k]
      # feature5: exact-context popularity
      ngram1 <- paste(c(minus2, minus1, candidate), collapse = " ")
      ngram2 <- paste(c(minus1, candidate, positive1), collapse = " ")
      ngram3 <- paste(c(candidate, positive1, positive2), collapse = " ")
      # candidate ngram freq (exact)
      candidate_frequency <- sum(freq5(ngram1, three_gram), 
                                 freq5(ngram2, three_gram),   
                                 freq5(ngram3, three_gram))
      frequency5 <- append(frequency5, candidate_frequency)
      
      # feature6: relaxed-context popularity
      ngram1_1 <- paste(c(".", minus1, candidate), collapse = " ")
      ngram1_2 <- paste(c(minus2, ".", candidate), collapse = " ")

      ngram2_1 <- paste(c(".", candidate, positive1), collapse = " ")
      ngram2_2 <- paste(c(minus1, candidate, "."), collapse = " ")
      
      ngram3_1 <- paste(c(candidate, ".", positive2), collapse = " ")
      ngram3_2 <- paste(c(candidate, positive1, "."), collapse = " ")
      # candidate ngram freq (relaxed)
      candidate_frequency <- sum(freq6(ngram1_1, three_gram), 
                                 freq6(ngram1_2, three_gram), 
                                 freq6(ngram2_1, three_gram), 
                                 freq6(ngram2_2, three_gram),
                                 freq6(ngram3_1, three_gram),
                                 freq6(ngram3_2, three_gram))
      frequency6 <- append(frequency6, candidate_frequency)
    }
    
    score_ecp <- context_score(frequency5)
    score_rcp <- context_score(frequency6)

    data_set <- add_row(data_set, "error_term" = rep(w_e, length(candidate_set)), 
                           "gt_term" = rep(gt_term[j], length(candidate_set)), 
                           "candidate" = candidate_set,
                           "feature1" = score_led,
                           "feature2" = score_ss,
                          "feature4" = score_le,
                           "feature3" = score_lp,
                           "feature5" = score_ecp,
                           "feature6" = score_rcp,
                        "file_index" = rep(i, length(candidate_set)),
                        "error_index" = rep(error_position[j], length(candidate_set)))

  }
# the loop for each file
data_set$y <- as.numeric(data_set$gt_term == data_set$candidate)
end_time_ds <- Sys.time()
# time_ds <- end_time_ds - start_time_ds ## file1, 3, 4, 7, 8; first 20 errors for each file; first 5 candidates for each error
print(paste("Running time for building dataset = ", (end_time_ds - start_time_ds)))
filename = paste(paste("dataset", i, sep = "_"), "RData", sep = ".")
save(data_set, file=paste(filepath, filename))
}

```

<<<<<<< HEAD

## Regressor
=======
## Regression
>>>>>>> d91af1b90e3970e9ac78422353bac35cd773bafb

### Select optimal parameters

```{r parameters}
# tuneFunc = function(df, feature_col = 5){
#   df1 = df[as.character(df$gt_term) == as.character(df$candidate),]
#   return(mean(df1[, feature_col]))
# }
# 
# ## feature 2
# f2_scores = rep(NA, 5)
# # combination 1 for feature2: a_1 = 0.5, a_2 = 0.25, a_3 = 0.25
# # combination 2 for feature2: a_1 = 0.2, a_2 = 0.2, a_3 = 0.6
# # combination 3 for feature2: a_1 = 0.4, a_2 = 0.4, a_3 = 0.2
# # combination 4 for feature2: a_1 = 0.5, a_2 = 0.25, a_3 = 0.25
# # combination 5 for feature2: a_1 = 0.3, a_2 = 0.5, a_3 = 0.2
# for(i in 1:4){
#   load(paste0("../output/data_set_f2_",i,".RData"))
#   head(data_set)
#   f2_scores[i] = tuneFunc(data_set, 5)
# }
# 
# load(paste0("../output/data_set_f2_1.RData"))
# ds1 = data_set
# load(paste0("../output/data_set_f2_2.RData"))
# ds2 = data_set
# load(paste0("../output/data_set_f2_3.RData"))
# ds3 = data_set
# load(paste0("../output/data_set_f2_4.RData"))
# ds4 = data_set
```


### Use AdaBoost.R2 Model on top of decision trees with 0-1 loss function.

```{r regressor}

set.seed(1)
## seperate train set and test set
## we split the data set to about 80% train set and 20% test set, file wise
dataset_vec <- list.files("../output/datasets/")
train_file = sample(dataset_vec, round(length(dataset_vec)*0.8))
test_file = setdiff(dataset_vec, train_file)
train_set = data.frame()
test_set = data.frame()
for(i in 1:length(train_file)){
  load(paste0("../output/datasets/", train_file[i]))
  train_set = rbind(train_set, data_set)
}
n_test = length(test_file)
test_ind = rep(NA, n_test)
for(i in 1:n_test){
  load(paste0("../output/datasets/", test_file[i]))
  test_set = rbind(test_set, data_set)
  name = strsplit(test_file[i], split =" dataset_")[[1]][2]
  test_ind[i] = strsplit(name, split = ".RData")[[1]]
}
train_set$factory = factor(train_set$y)
test_set$factory = factor(test_set$y)
test_ind = as.numeric(test_ind)

## train regressor
reg = adam2(factory~feature1+feature2+feature3+feature4+feature5+feature6, data = train_set, size = 10, alg = "c50")

## regression on the test set and rank candidates
num_candidate = 3 # number of candidate for each error
test_prob = predict(reg, newdata = test_set)
test_outcome = data.frame(prob = test_prob, 
                          error_term = test_set$error_term, gt_term = test_set$gt_term, candidate = test_set$candidate, 
                          file_index = test_set$file_index, error_index = test_set$error_index,
                          pair = rep(1:(nrow(test_set)/num_candidate), each = num_candidate)) %>%
  group_by(pair) %>%
  mutate(Rank = rank(prob, ties.method = "first")) %>%
  mutate(Rank = num_candidate + 1 - Rank)

## select the top 1 candidate as our correction
selected_correction = test_outcome[test_outcome$Rank == 1,]
save(selected_correction, file = "../output/selected_correction.RData")
save(test_outcome, file = "../output/test_outcome.RData")

```

# Step 6 - Performance measure

The two most common OCR accuracy measures are precision and recall. Both are relative measures of the OCR accuracy because they are computed as ratios of the correct output to the total output (precision) or input (recall). More formally defined,
\begin{align*}
\mbox{precision}&=\frac{\mbox{number of correct items}}{\mbox{number of items in OCR output}}\\
\mbox{recall}&=\frac{\mbox{number of correct items}}{\mbox{number of items in ground truth}}
\end{align*}
where *items* refer to either characters or words, and ground truth is the original text stored in the plain text file. 

## Evaluation of Detection

### Word Level Precision & Recall 
```{r detection word level}
load("../output/MatchDetectCharacter.RData")
precisionDE<-rep(0,100)
precisionBM<-rep(0,100)

recallDE<-rep(0,100)
recallBM<-rep(0,100)

intersect_vec<-list()

for (i in 1:100) {
  ## precison and recall based on detection output
  precisionDE[i]<-sum(as.logical(unlist(MatchDetectCharacter[[i]]$Detect)))/word_num_tes[i]
  recallDE[i]<-sum(as.logical(unlist(MatchDetectCharacter[[i]]$Detect)))/word_num_gt[i]
  
  ## precision and recall based on Tesseract files
  intersect_vec[i]<- length(vecsets::vintersect(tolower(unlist(gt_vec_all[i])),tolower(unlist(tesseract_vec_all[i]))))
  precisionBM[i]<-as.numeric(intersect_vec[i])/word_num_tes[i]
  recallBM[i]<-as.numeric(intersect_vec[i])/word_num_gt[i]
}

## visualize precision comparison
pDetectCompare = data.frame(precision = c(precisionDE, precisionBM),
                            file = rep(c(1:n_file), 2),
                            category = rep(c("detection","Tesseract"),each = 100))
ggplot(NULL) +
  geom_line(aes(x = file, y = precision, color = category), data = pDetectCompare) +
  xlab("file number") +
  ylab("precisions") + 
  ggtitle("Precision Comparison")

## visualize recall comparison
rDetectCompare = data.frame(recall = c(recallDE, recallBM),
                            file = rep(c(1:n_file), 2),
                            category = rep(c("detection","Tesseract"),each = 100))
ggplot(NULL) +
  geom_line(aes(x = file, y = recall, color = category), data = rDetectCompare) +
  xlab("file number") +
  ylab("recall") +
  ggtitle("Recall Comparison")

<<<<<<< HEAD

=======
## form a table for result
>>>>>>> d91af1b90e3970e9ac78422353bac35cd773bafb
Detection_performance_table <- data.frame("Tesseract" = rep(NA,2),
                                    "Detection" = rep(NA,2))
row.names(Detection_performance_table) = c("word_wise_recall","word_wise_precision")
Detection_performance_table["word_wise_recall","Tesseract"] <- mean(recallBM)
Detection_performance_table["word_wise_precision","Tesseract"] <- mean(precisionBM)
Detection_performance_table["word_wise_recall","Detection"] <- mean(recallDE)
Detection_performance_table["word_wise_precision","Detection"] <- mean(precisionDE)
kable(Detection_performance_table, caption="Comparison Between Tesseract & Detection")
```

If the detection algorithm works well, the red line and blue line should be close to each other. But in our output they are not exactly the same, but we can see that red line and blue line in both pictures have the same trend.

### Confusion Matrix and Other KPI
```{r detection confusion matrix}
load("../output/MatchDetectCharacter.RData")
CompareMD = MatchDetectCharacter
compareFunc = function(df){
  df$Actual = df$Tesseract == df$GroundTruth
  return(df)
}

CompareDetect = lapply(CompareMD, compareFunc) # each element is a dataframe with a new column of actual detection
## merge the detect and actual comparison of all 100 files into 1 matric
CompareTF = cbind(unlist(sapply(CompareDetect, `[[`, "Detect")), unlist(sapply(CompareDetect, `[[`, "Actual"))) %>%
  data.frame()
colnames(CompareTF) = c("Detect", "Actual")

## confusion matrix calculation
confusionMatrix(data = CompareTF$Detect, reference = CompareTF$Actual)
```

Then we construct a confusion matrix, and then we found that the major problem of our detection algorithm is false positive, which is a true error indicated by ground truth but can not be detected by our detection method.

We believed that it is probably because this rule based method is rather intuitive, not context-adjusted. Adding more rules or put different weight in each rules may be helpful.

## Evaluation of Correction

### Word Level & Character Level Precision & Recall 
```{r correction word level}
load("../output/selected_correction.RData")
load("../output/test_outcome.RData")

OCR_performance_table <- data.frame("Tesseract" = rep(NA,4),
                                    "PostProcessing" = rep(NA,4))
row.names(OCR_performance_table) = c("word_wise_recall","word_wise_precision",
                                    "character_wise_recall", "character_wise_precision")

##################
### Word Level ###
##################

##### Measurements on Tesseract Files #####
wr_tes = rep(NA, n_file)
wp_tes = rep(NA, n_file)
for(i in 1:n_file){
  tes_word = vecsets::vintersect(tolower(gt_vec_all[[i]]), tolower(tesseract_vec_all[[i]]))
  wr_tes[i] = length(tes_word)/word_num_gt[i]
  wp_tes[i] = length(tes_word)/word_num_tes[i]
}
OCR_performance_table["word_wise_recall","Tesseract"] <- mean(wr_tes)
OCR_performance_table["word_wise_precision","Tesseract"] <- mean(wp_tes)

##### Measurements on Corrected Files #####
correct_vec_all = list()
wr_correct = rep(NA, n_test)
wp_correct = rep(NA, n_test)
for(i in 1:n_test){
  # restore our correction result to the text
  file_ind = test_ind[i]
  correct_current = selected_correction[selected_correction$file_index == file_ind, ]
  correct_vec_current = tesseract_vec_all[[file_ind]]
  correct_vec_current[correct_current$error_index] = correct_current$candidate
  correct_vec_all[[i]] = correct_vec_current
  correct_word = vecsets::vintersect(tolower(gt_vec_all[[file_ind]]), tolower(correct_vec_current))
  wr_correct[i] = length(correct_word)/word_num_gt[file_ind]
  wp_correct[i] = length(correct_word)/word_num_tes[file_ind]
}
OCR_performance_table["word_wise_recall","PostProcessing"] <- mean(wr_correct)
OCR_performance_table["word_wise_precision","PostProcessing"] <- mean(wp_correct)

#######################
### Character Level ###
#######################
chars = c(letters,0,1,2,3,4,5,6,7,8,9)
recallFunc = function(nums){
    text = nums[1]
    gt = nums[2]
    return(min(text,gt)/max(text,gt))
}
precisionFunc = function(nums){
    text = nums[1]
    gt = nums[2]
    return(min(text,gt)/text)
}
##### Measurements on Tesseract Files #####
lr_tes = rep(NA, n_file)
lp_tes = rep(NA, n_file)
for(i in 1:n_file){
  tes_letter = tolower(unlist(strsplit(tesseract_vec_all[[i]], split = "")))
  tes_count = table(tes_letter)
  names1 = names(tes_count)
  gt_letter = tolower(unlist(strsplit(gt_vec_all[[i]], split = "")))
  gt_count = table(gt_letter)
  names2 = names(gt_count)
  
  char_current = intersect(intersect(names1, names2), chars)
  
  count_logi1 = names(tes_count) %in% char_current
  tes_count = tes_count[count_logi1]
  count_logi2 = names(gt_count) %in% char_current
  gt_count = gt_count[count_logi2]
  
  counts = rbind(tes_count, gt_count)
  
  lr_tes[i] = mean(apply(counts, 2, recallFunc))
  lp_tes[i] = mean(apply(counts, 2, precisionFunc))
}
OCR_performance_table["character_wise_recall","Tesseract"] <- mean(lr_tes)
OCR_performance_table["character_wise_precision","Tesseract"] <- mean(lp_tes)

##### Measurements on Corrected Files #####
lr_correct = rep(NA, n_test)
lp_correct = rep(NA, n_test)
for(i in 1:n_test){
  file_ind = test_ind[i]
  correct_letter = tolower(unlist(strsplit(correct_vec_all[[i]], split = "")))
  correct_count = table(correct_letter)
  names1 = names(correct_count)
  gt_letter = tolower(unlist(strsplit(gt_vec_all[[file_ind]], split = "")))
  gt_count = table(gt_letter)
  names2 = names(gt_count)
  
  char_current = intersect(intersect(names1, names2), chars)
  
  count_logi1 = names(correct_count) %in% char_current
  correct_count = correct_count[count_logi1]
  count_logi2 = names(gt_count) %in% char_current
  gt_count = gt_count[count_logi2]
  
  counts = rbind(correct_count, gt_count)
  
  lr_correct[i] = mean(apply(counts, 2, recallFunc))
  lp_correct[i] = mean(apply(counts, 2, precisionFunc))
}
OCR_performance_table["character_wise_recall","PostProcessing"] <- mean(lr_correct)
OCR_performance_table["character_wise_precision","PostProcessing"] <- mean(lp_correct)

kable(OCR_performance_table, caption="Comparison Between Tesseract & Post Processing")
```

After calculation we find that our post-processing process can make an enhancement on our OCR result. Word and character level of precision and recall is improved.

### Top 3 Candidates Coverage

In this part we calculate what is the proportion of errors that can be corrected by top 1, top 2 and top 3 candidates.

```{r top 3}
load("../output/test_outcome.RData")
test_outcome_listed = test_outcome %>%
  group_by(file_index, error_index) %>%
  mutate(c1 = candidate[Rank == 1],
         c2 = candidate[Rank == 2],
         c3 = candidate[Rank == 3]) %>%
  select(candidate,gt_term,file_index,error_index,c1,c2,c3,Rank) %>%
  filter(Rank == 1) %>%
  mutate(gt_term = as.character(gt_term), candidate = as.character(candidate),
         c1 = as.character(c1), c2 = as.character(c2), c3 = as.character(c3))

n_row = nrow(test_outcome_listed)
test_coverage = test_outcome_listed %>%
  group_by(file_index, error_index) %>%
  mutate(cover1 = gt_term%in%c1,
         cover2 = gt_term%in%c(c1,c2),
         cover3 = gt_term%in%c(c1,c2,c3))
cover_prop1 = sum(test_coverage$cover1)/n_row
cover_prop2 = sum(test_coverage$cover2)/n_row
cover_prop3 = sum(test_coverage$cover3)/n_row
cover_result = data.frame("top 1" = cover_prop1, 
                          "top 2" = cover_prop2, 
                          "top 3" = cover_prop3)
row.names(cover_result) = "test set"
kable(cover_result, caption="Top 3 Candidates Coverage")
```

From this indicator we can see that more than 59% of error can be corrected by top 3 candidates provided by our correction method.

### Time Consuming

We timed our detection process and feature scoring process,

Detection process shows that:

The CPU time charged for the execution: 1909.355
The CPU time charged for the execution of user instructions: 484.844

Feature Scoring process shows that: on average it takes about 7.3767 minute to complete a text.
