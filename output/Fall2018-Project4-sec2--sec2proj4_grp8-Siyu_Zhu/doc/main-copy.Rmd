---
title: 'Optical character recognition (OCR)'
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: false
    number_sections: true
    code_folding: show
---
# Step 1 - Load library and source code
```{r, warning=FALSE, message = FALSE}
# if (!require("devtools")) install.packages("devtools")
# if (!require("pacman")) {
#   ## devtools is required
#   library(devtools)
#   install_github("trinker/pacman")
# }
# 
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
library(tm)
library(RWeka)
library(ggplot2)
library(slam)
load("../output/unigram.RData")
load("../output/three_gram.RData")
library('stringdist')

file_name_vec <- list.files("../data/ground_truth") #100 files in total
```

# Step 3 - Error detection

## Data preparation
```{r}
# read ground truth text into 1 list
n_file = length(file_name_vec)
tesseract_vec_all = list() # each element is a vector of words for each text file
tesseract_lines_all = list() # each element is a list of lines for each text file
gt_vec_all = list() # each element is a vector of words for each text file
gt_lines_all = list() # each element is a list of lines for each text file
len_check = rep(NA, 100)
len_check2 = rep(NA, 100)

## check if each tesseract file has the same number of length with its corresponding ground truth

for(i in 1:n_file){
  ## i represents that this is the i-th file we are dealing with
  current_file_name <- sub(".txt","",file_name_vec[i])
  
  ## read the ground truth text, save it by lines & vectorize it
  current_ground_truth_txt = readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)

  ## read the tesseract text, save it by lines & vectorize it
  current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)
  
  ## check if each tesseract file has the same number of length with its corresponding ground truth
  len_check[i] = length(current_ground_truth_txt) == length(current_tesseract_txt)
  ## check if all ground truth are not shorter than its Tesseract
  len_check2[i] = length(current_ground_truth_txt) >= length(current_tesseract_txt)
}

# there exist pairs that do not have same number of lines:
which(!len_check) 
# all ground truth files have at least same number of lines compared with corresponding Tesseract files:
which(!len_check2) 
n_file-sum(len_check)
```

The above chunck shows that 13 pairs of Tesseract and ground truth files do not have the same number of lines, and for such pairs, the ground truth files always have more lines than the Tesseract files.

Therefore, we trimmed the ground truth files manually until they have the same length with the corresponding Tesseract files, and saved them in a new folder called "ground_truth_trimmed". 

To simplify the word alignment process, we only work on the line pairs that have same number of words.

All the process stated below are based on trimmed files, same length line pairs.

```{r}
same_na = rep(NA, n_file)
for(i in 1:n_file){
  ## i represents that this is the i-th file we are dealing with
  current_file_name <- sub(".txt","",file_name_vec[i])
  
  ## read the ground truth text
  current_ground_truth_txt = readLines(paste("../data/ground_truth_trimmed/",current_file_name,".txt",sep=""), warn=FALSE)
  
  ## read the tesseract text
  current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)

  ## only keep the line pairs that have the same number of words
  n_line = length(current_ground_truth_txt)
  for (j in 1:n_line){
    same_len_logi = length(str_split(current_ground_truth_txt[j], " ")[[1]]) == 
      length(str_split(current_tesseract_txt[j], " ")[[1]])
    if(!same_len_logi){
      current_ground_truth_txt[j] = NA
      current_tesseract_txt[j] = NA
    }
  }
  na_logi1 = is.na(current_ground_truth_txt)
  na_logi2 = is.na(current_tesseract_txt)
  same_na[i] = all(na_logi1 == na_logi2) # check if all the lines are assigned accordingly
  
  ## save and vectorized cleaned text
  gt_lines_all[[i]] = current_ground_truth_txt[!na_logi1]
  gt_vec_all[[i]] <- str_split(paste(gt_lines_all[[i]], collapse = " ")," ")[[1]]
  tesseract_lines_all[[i]] = current_tesseract_txt[!na_logi2]
  tesseract_vec_all[[i]] = str_split(paste(tesseract_lines_all[[i]], collapse = " ")," ")[[1]]
  
  ## re-check if each tesseract file has the same number of length with its corresponding ground truth
  len_check[i] = length(tesseract_lines_all[[i]]) == length(gt_lines_all[[i]])
}
## recheck if lines are removed correctly
all(same_na)
## re-check if each tesseract file has the same number of length with its corresponding ground truth
all(len_check)

## recheck if all the words in a pair of files can be mapped 1 to 1 now
map11 = rep(NA,n_file)
for(i in 1:n_file){
  map11[i] = length(tesseract_vec_all[[i]]) == length(gt_vec_all[[i]])
}
all(map11)
```


## Detection
Each element in the list in this following chunck is a dataframe for a pair of files. Each row in a dataframe contains the Tesseract word, its ground truth word and the detect output.

```{r}
MatchDetect = list()
for(i in 1:n_file){
  tesseract_vec <- tesseract_vec_all[[i]]
  tesseract_if_clean <- unlist(lapply(tesseract_vec,ifCleanToken)) # source code of ifCleanToken in in lib folder
  mat = cbind(tesseract_vec_all[[i]], gt_vec_all[[i]],tesseract_if_clean)
  df = data.frame(mat)
  colnames(df) = c("Tesseract", "GroundTruth", "Detect")
  MatchDetect[[i]] = df
}
save(MatchDetect, file=paste0("../output/MatchDetect.RData"))
```

# Step 4 - Error correction
## build ground_truth file
```{r}
load("../output/MatchDetect.RData")
## ground truth string file
ground_truth <- NULL
ground_truth <- paste(unlist(gt_lines_all), collapse = " ")
ground_truth_vec <- str_split(ground_truth," ")[[1]] 
```

## build ngram frequency vocabulary
```{r}

ground_truth <- iconv(ground_truth, "latin1", "ASCII", sub="") # pre_clean data
ground_truth <- iconv(ground_truth,to="utf-8-mac")
ground_truth_Corpus <- VCorpus(VectorSource(ground_truth)) # Make corpus
ground_truth_Corpus <- tm_map(ground_truth_Corpus, stripWhitespace) # Remove unneccesary white spaces
ground_truth_Corpus <- tm_map(ground_truth_Corpus, removePunctuation) # Remove punctuation
ground_truth_Corpus <- tm_map(ground_truth_Corpus, removeNumbers) # Remove numbers
ground_truth_Corpus <- tm_map(ground_truth_Corpus, tolower) # Convert to lowercase
ground_truth_Corpus <- tm_map(ground_truth_Corpus, PlainTextDocument) # Plain text

n_grams_vocabulary <- function(n, data) {
  options(mc.cores=1)
  # Builds n-gram tokenizer 
  tk <- function(x) NGramTokenizer(x, Weka_control(min = n, max = n))
  # Create matrix
  ngrams_matrix <- TermDocumentMatrix(data, control=list(tokenize=tk))
  # make matrix for easy view
  ngrams_matrix <- as.matrix(rollup(ngrams_matrix, 2, na.rm=TRUE, FUN=sum))
  ngrams_matrix <- data.frame(word=rownames(ngrams_matrix), freq=ngrams_matrix[,1])
  # find 20 most frequent n-grams in the matrix
  ngrams_matrix <- ngrams_matrix[order(-ngrams_matrix$freq), ]
  ngrams_matrix$word <- factor(ngrams_matrix$word, as.character(ngrams_matrix$word))
  return(ngrams_matrix)
}

unigram <- n_grams_vocabulary(n=1, data=ground_truth_Corpus)
three_gram <- n_grams_vocabulary(n=3, data=ground_truth_Corpus)
save(unigram, file=paste0("../output/unigram.RData"))
save(three_gram, file=paste0("../output/three_gram.RData"))

# plots first 20 three_gram
ggplot(three_gram[1:20,], aes(x=word, y=freq)) + 
  geom_bar(stat="Identity") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab("three-grams") + 
  ylab("Frequency")
```

## build features, create dataset
```{r}
Sys.time()
correct_pattern <- function(single_pattern){
  return(gsub(x = single_pattern, pattern = "\\(", replacement = "\\\\\\(", ignore.case = TRUE))
}


initial_dataset <- function(){
  data_set <- data.frame("error_term" = character(0), 
                           "gt_term" = character(0), 
                           "candidate" = character(0),
                           "feature1" = integer(0),
                           "feature2" = integer(0),
                            "feature3" = integer(0),
                            "feature4" = integer(0),
                           "feature5" = integer(0),
                           "feature6" = integer(0),
                           "file_index" = integer(0),
                           "error_index" = integer(0))
  return(data_set)
  
}



## function to calcualte candidate's exact frequency
freq5 <- function(test, n_gram){
  ifelse(sum(n_gram$word == test) == 0, 0, n_gram[n_gram$word == test, ]$freq)
}

## function to calcualte candidate's relaxd frequency
freq6 <- function(test, ngram){
  test <- correct_pattern(test)
  ifelse(sum(grep(test, ngram$word)) == 0, 0, sum(ngram$freq[grep(test, ngram$word)]))
}

## function to calculate score based on context frequency
context_score <- function(frequency){
  if(max(frequency) == 0){
    return(frequency)
  }else{
    return(frequency/max(frequency))
  } 
}


# parameter of feature1
delta <- 5
# first s elements we keep in the candidate_set
s <- 3

source("../lib/string_similarity.R") # used in calculating feature2

#### initial the filepath for storing the output
filepath = "../output/"
## for each file  #length(tesseract_lines_all), c(1, 3, 4, 7, 8)
for (i in c(1:10)) {
  
  data_set = initial_dataset()
  start_time_ds <- Sys.time()
  ## create lexicon for candidate search
  # ground_truth lexicon
  current_gt <- paste(gt_lines_all[[i]], collapse = " ")
  current_gt_vec <- str_split(current_gt, " ")[[1]]
  
  additional_file <- sample((1:100)[-i],  1)
  all_gt <- paste(c(gt_lines_all[[i]], 
                    gt_lines_all[[additional_file[1]]]), collapse = " ")
  all_gt_vec <- str_split(all_gt," ")[[1]] 
  all_gt_vec <- unique(all_gt_vec)
  
  ## search error_term and corresponding ground truth for each file
  error_position <- which(MatchDetect[[i]]$Detect == FALSE)
  error_term <- as.character(MatchDetect[[i]]$Tesseract[error_position])
  gt_term <- as.character(MatchDetect[[i]]$GroundTruth[error_position])

  ## for each error  #length(error_term)
  for (j in 1:length(error_term)){
    w_e <- error_term[j] # word_error
    ## create a candidate set for each error
    leven_distance <- sapply(all_gt_vec, stringdist, b = w_e, method = "lv") # calculate Levenshtein distance between error term and each candidate in all_gt_vec
    chosen_candidate <- order(leven_distance)[1:s] # chose first s candidates position
    candidate_set <- all_gt_vec[chosen_candidate]
    
    ## feature scoring
    
    ### feature1: Levenshtein edit distance
    # Levenshtein distance for the candidate
    dist_candidate <- as.numeric(leven_distance[chosen_candidate])
    score_led <- 1 - dist_candidate/(delta + 1)
    
    ### feature2: String similarity
    score_ss <- string_similiarity(w_e, candidate_set)
    
    ### feature3: Language popularity
    frequency3 <- unlist(lapply(candidate_set, freq5, unigram))
    score_lp <- context_score(frequency3)
    
    ### feature4: lexicon existance
    score_le <- as.numeric(candidate_set%in%current_gt_vec)
    
    ### feature5 and 6: exact-context popularity andrelaxed-context popularity
    # neighboring terms
    minus2 <-  as.character(MatchDetect[[i]]$Tesseract[error_position[j] - 2])
    minus1 <- as.character(MatchDetect[[i]]$Tesseract[error_position[j] - 1])
    positive1 <-  as.character(MatchDetect[[i]]$Tesseract[error_position[j] + 1])
    positive2 <-  as.character(MatchDetect[[i]]$Tesseract[error_position[j] + 2])
    frequency5 <- frequency6 <- NULL
    ## feature5 and feature6
    # for each candidate, calculate ngram frequency 
    for (k in 1:length(candidate_set)) {
      # tested candidate
      candidate <- candidate_set[k]
      # feature5: exact-context popularity
      ngram1 <- paste(c(minus2, minus1, candidate), collapse = " ")
      ngram2 <- paste(c(minus1, candidate, positive1), collapse = " ")
      ngram3 <- paste(c(candidate, positive1, positive2), collapse = " ")
      # candidate ngram freq (exact)
      candidate_frequency <- sum(freq5(ngram1, three_gram), 
                                 freq5(ngram2, three_gram),   
                                 freq5(ngram3, three_gram))
      frequency5 <- append(frequency5, candidate_frequency)
      
      # feature6: relaxed-context popularity
      ngram1_1 <- paste(c(".", minus1, candidate), collapse = " ")
      ngram1_2 <- paste(c(minus2, ".", candidate), collapse = " ")

      ngram2_1 <- paste(c(".", candidate, positive1), collapse = " ")
      ngram2_2 <- paste(c(minus1, candidate, "."), collapse = " ")
      
      ngram3_1 <- paste(c(candidate, ".", positive2), collapse = " ")
      ngram3_2 <- paste(c(candidate, positive1, "."), collapse = " ")
      # candidate ngram freq (relaxed)
      candidate_frequency <- sum(freq6(ngram1_1, three_gram), 
                                 freq6(ngram1_2, three_gram), 
                                 freq6(ngram2_1, three_gram), 
                                 freq6(ngram2_2, three_gram),
                                 freq6(ngram3_1, three_gram),
                                 freq6(ngram3_2, three_gram))
      frequency6 <- append(frequency6, candidate_frequency)
    }
    
    score_ecp <- context_score(frequency5)
    score_rcp <- context_score(frequency6)

    data_set <- add_row(data_set, "error_term" = rep(w_e, length(candidate_set)), 
                           "gt_term" = rep(gt_term[j], length(candidate_set)), 
                           "candidate" = candidate_set,
                           "feature1" = score_led,
                           "feature2" = score_ss,
                          "feature4" = score_le,
                           "feature3" = score_lp,
                           "feature5" = score_ecp,
                           "feature6" = score_rcp,
                        "file_index" = rep(i, length(candidate_set)),
                        "error_index" = rep(error_position[j], length(candidate_set)))

  }
# the loop for each file
data_set$y <- as.numeric(data_set$gt_term == data_set$candidate)
end_time_ds <- Sys.time()
# time_ds <- end_time_ds - start_time_ds ## file1, 3, 4, 7, 8; first 20 errors for each file; first 5 candidates for each error
print(paste("Running time for building dataset = ", (end_time_ds - start_time_ds)))
filename = paste(paste("dataset", i, sep = "_"), "RData", sep = ".")
save(data_set, file=paste(filepath, filename))
}
# 
# data_set$y <- as.numeric(data_set$gt_term == data_set$candidate)
# end_time_ds <- Sys.time()
# time_ds <- end_time_ds - start_time_ds ## file1, 3, 4, 7, 8; first 20 errors for each file; first 5 candidates for each error
# print(paste("Running time for building dataset = ", time_ds))
# filepath = "../output/"


```


