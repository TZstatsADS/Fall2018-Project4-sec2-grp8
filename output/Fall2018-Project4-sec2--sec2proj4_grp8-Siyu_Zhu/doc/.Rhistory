"feature3" = score_lp,
"feature5" = score_ecp,
"feature6" = score_rcp,
"file_index" = rep(i, length(candidate_set)),
"error_index" = rep(error_position[j], length(candidate_set)))
}
}
j
k
ngram1
ngram2
ngram1_1
ngram1_2
ngram2_1
ngram2_2
ngram3_1
freq6(ngram1_1, three_gram)
freq6(ngram2_1, three_gram)
freq6(ngram2_2, three_gram)
freq6(ngram3_1, three_gram)
ngram3_1
candidate
positive2
ngram2
positive2
ngram3
freq5(ngram3, three_gram)
grep("(gram3", three_gram)
grep("\(gram3", three_gram)
grep("/(gram3", three_gram)
regex("(gram3")
grep("/(gram3", three_gram, perl = TRUE)
grep("(gram3", three_gram, perl = TRUE)
ngram3_1
ngram1_1
ngram3_1
ngram2_1
ngram2_2
correct_pattern <- function(single_pattern){
gsub(x = pattern, pattern = "(", replacement = "\(")
return(gsub(x = single_pattern, pattern = "(", replacement = "\("))
correct_pattern <- function(single_pattern){
return(gsub(x = single_pattern, pattern = "(", replacement = "\("))
return(gsub(x = single_pattern, pattern = "(", replacement = "\(", ignore.case = TRUE))
correct_pattern <- function(single_pattern){
return(gsub(x = single_pattern, pattern = "(", replacement = "\(", ignore.case = TRUE))
correct_pattern <- function(single_pattern){
return(gsub(x = single_pattern, pattern = "(", replacement = ".", ignore.case = TRUE))
}
grep(correct_pattern(ngram3_1), three_gram)
correct_pattern(ngram3_1)
correct_pattern <- function(single_pattern){
return(gsub(x = single_pattern, pattern = "\(", replacement = ".", ignore.case = TRUE))
correct_pattern <- function(single_pattern){
return(gsub(x = single_pattern, pattern = "\\(", replacement = ".", ignore.case = TRUE))
}
correct_pattern(ngram3_1)
correct_pattern <- function(single_pattern){
return(gsub(x = single_pattern, pattern = "\\(", replacement = ".", ignore.case = TRUE))
}
correct_pattern <- function(single_pattern){
return(gsub(x = single_pattern, pattern = "\\(", replacement = "\\(", ignore.case = TRUE))
}
grep(correct_pattern(ngram3_1), three_gram)
correct_pattern <- function(single_pattern){
return(gsub(x = single_pattern, pattern = "\\(", replacement = "\\\\\\(", ignore.case = TRUE))
}
grep(correct_pattern(ngram3_1), three_gram)
correct_pattern(ngram3_1)
## function to calcualte candidate's relaxd frequency
freq6 <- function(test, ngram){
test <- correct_pattern(test)
ifelse(sum(grep(test, ngram$word)) == 0, 0, sum(ngram$freq[grep(test, ngram$word)]))
}
load("../output/unigram.RData")
load("../output/three_gram.RData")
library('stringdist')
correct_pattern <- function(single_pattern){
return(gsub(x = single_pattern, pattern = "\\(", replacement = "\\\\\\(", ignore.case = TRUE))
}
# grep(correct_pattern(ngram3_1), three_gram)
data_set <- data.frame("error_term" = character(0),
"gt_term" = character(0),
"candidate" = character(0),
"feature1" = integer(0),
"feature2" = integer(0),
"feature3" = integer(0),
"feature4" = integer(0),
"feature5" = integer(0),
"feature6" = integer(0),
"file_index" = integer(0),
"error_index" = integer(0))
## function to calcualte candidate's exact frequency
freq5 <- function(test, n_gram){
ifelse(sum(n_gram$word == test) == 0, 0, n_gram[n_gram$word == test, ]$freq)
}
## function to calcualte candidate's relaxd frequency
freq6 <- function(test, ngram){
test <- correct_pattern(test)
ifelse(sum(grep(test, ngram$word)) == 0, 0, sum(ngram$freq[grep(test, ngram$word)]))
}
## function to calculate score based on context frequency
context_score <- function(frequency){
if(max(frequency) == 0){
return(frequency)
}else{
return(frequency/max(frequency))
}
}
# parameter of feature1
delta <- 5
# first s elements we keep in the candidate_set
s <- 3
source("../lib/string_similarity.R") # used in calculating feature2
### create dataset
start_time_ds <- Sys.time()
## for each file  #length(tesseract_lines_all), c(1, 3, 4, 7, 8)
for (i in 1) {
## create lexicon for candidate search
# ground_truth lexicon
current_gt <- paste(gt_lines_all[[i]], collapse = " ")
current_gt_vec <- str_split(current_gt, " ")[[1]]
additional_file <- sample((1:100)[-i],  1)
all_gt <- paste(c(gt_lines_all[[i]],
gt_lines_all[[additional_file[1]]]), collapse = " ")
all_gt_vec <- str_split(all_gt," ")[[1]]
all_gt_vec <- unique(all_gt_vec)
## search error_term and corresponding ground truth for each file
error_position <- which(MatchDetect[[i]]$Detect == FALSE)
error_term <- as.character(MatchDetect[[i]]$Tesseract[error_position])
gt_term <- as.character(MatchDetect[[i]]$GroundTruth[error_position])
## for each error  #length(error_term)
for (j in 1:length(error_term)){
w_e <- error_term[j] # word_error
## create a candidate set for each error
leven_distance <- sapply(all_gt_vec, stringdist, b = w_e, method = "lv") # calculate Levenshtein distance between error term and each candidate in all_gt_vec
chosen_candidate <- order(leven_distance)[1:s] # chose first s candidates position
candidate_set <- all_gt_vec[chosen_candidate]
## feature scoring
### feature1: Levenshtein edit distance
# Levenshtein distance for the candidate
dist_candidate <- as.numeric(leven_distance[chosen_candidate])
score_led <- 1 - dist_candidate/(delta + 1)
### feature2: String similarity
score_ss <- string_similiarity(w_e, candidate_set)
### feature3: Language popularity
frequency3 <- unlist(lapply(candidate_set, freq5, unigram))
score_lp <- context_score(frequency3)
### feature4: lexicon existance
score_le <- as.numeric(candidate_set%in%current_gt_vec)
### feature5 and 6: exact-context popularity andrelaxed-context popularity
# neighboring terms
minus2 <-  as.character(MatchDetect[[i]]$Tesseract[error_position[j] - 2])
minus1 <- as.character(MatchDetect[[i]]$Tesseract[error_position[j] - 1])
positive1 <-  as.character(MatchDetect[[i]]$Tesseract[error_position[j] + 1])
positive2 <-  as.character(MatchDetect[[i]]$Tesseract[error_position[j] + 2])
frequency5 <- frequency6 <- NULL
## feature5 and feature6
# for each candidate, calculate ngram frequency
for (k in 1:length(candidate_set)) {
# tested candidate
candidate <- candidate_set[k]
# feature5: exact-context popularity
ngram1 <- paste(c(minus2, minus1, candidate), collapse = " ")
ngram2 <- paste(c(minus1, candidate, positive1), collapse = " ")
ngram3 <- paste(c(candidate, positive1, positive2), collapse = " ")
# candidate ngram freq (exact)
candidate_frequency <- sum(freq5(ngram1, three_gram),
freq5(ngram2, three_gram),
freq5(ngram3, three_gram))
frequency5 <- append(frequency5, candidate_frequency)
# feature6: relaxed-context popularity
ngram1_1 <- paste(c(".", minus1, candidate), collapse = " ")
ngram1_2 <- paste(c(minus2, ".", candidate), collapse = " ")
ngram2_1 <- paste(c(".", candidate, positive1), collapse = " ")
ngram2_2 <- paste(c(minus1, candidate, "."), collapse = " ")
ngram3_1 <- paste(c(candidate, ".", positive2), collapse = " ")
ngram3_2 <- paste(c(candidate, positive1, "."), collapse = " ")
# candidate ngram freq (relaxed)
candidate_frequency <- sum(freq6(ngram1_1, three_gram),
freq6(ngram1_2, three_gram),
freq6(ngram2_1, three_gram),
freq6(ngram2_2, three_gram),
freq6(ngram3_1, three_gram),
freq6(ngram3_2, three_gram))
frequency6 <- append(frequency6, candidate_frequency)
}
score_ecp <- context_score(frequency5)
score_rcp <- context_score(frequency6)
data_set <- add_row(data_set, "error_term" = rep(w_e, length(candidate_set)),
"gt_term" = rep(gt_term[j], length(candidate_set)),
"candidate" = candidate_set,
"feature1" = score_led,
"feature2" = score_ss,
"feature4" = score_le,
"feature3" = score_lp,
"feature5" = score_ecp,
"feature6" = score_rcp,
"file_index" = rep(i, length(candidate_set)),
"error_index" = rep(error_position[j], length(candidate_set)))
}
}
data_set$y <- as.numeric(data_set$gt_term == data_set$candidate)
end_time_ds <- Sys.time()
time_ds <- end_time_ds - start_time_ds ## file1, 3, 4, 7, 8; first 20 errors for each file; first 5 candidates for each error
print(paste("Running time for building dataset = ", time_ds))
save(data_set, file=paste0("../output/data_set.RData"))
time_ds
data_set
data_set
data_set$error_index
nrow(data_set)
paste0("../output/data_set.RData", "i")
i = 1
filename = paste("dataset", i)
#### initial the filepath for storing the output
filepath = "../output/"
paste(filepath+filename)
paste(filepath, filename)
paste(filepath, filename, sep  "")
paste(filepath, filename, sep = "")
# grep(correct_pattern(ngram3_1), three_gram)
initial_dataset <- function(){
data_set <- data.frame("error_term" = character(0),
"gt_term" = character(0),
"candidate" = character(0),
"feature1" = integer(0),
"feature2" = integer(0),
"feature3" = integer(0),
"feature4" = integer(0),
"feature5" = integer(0),
"feature6" = integer(0),
"file_index" = integer(0),
"error_index" = integer(0))
}
load("../output/unigram.RData")
load("../output/three_gram.RData")
library('stringdist')
correct_pattern <- function(single_pattern){
return(gsub(x = single_pattern, pattern = "\\(", replacement = "\\\\\\(", ignore.case = TRUE))
}
initial_dataset <- function(){
data_set <- data.frame("error_term" = character(0),
"gt_term" = character(0),
"candidate" = character(0),
"feature1" = integer(0),
"feature2" = integer(0),
"feature3" = integer(0),
"feature4" = integer(0),
"feature5" = integer(0),
"feature6" = integer(0),
"file_index" = integer(0),
"error_index" = integer(0))
}
## function to calcualte candidate's exact frequency
freq5 <- function(test, n_gram){
ifelse(sum(n_gram$word == test) == 0, 0, n_gram[n_gram$word == test, ]$freq)
}
## function to calcualte candidate's relaxd frequency
freq6 <- function(test, ngram){
test <- correct_pattern(test)
ifelse(sum(grep(test, ngram$word)) == 0, 0, sum(ngram$freq[grep(test, ngram$word)]))
}
## function to calculate score based on context frequency
context_score <- function(frequency){
if(max(frequency) == 0){
return(frequency)
}else{
return(frequency/max(frequency))
}
}
# parameter of feature1
delta <- 5
# first s elements we keep in the candidate_set
s <- 3
source("../lib/string_similarity.R") # used in calculating feature2
#### initial the filepath for storing the output
filepath = "../output/"
## for each file  #length(tesseract_lines_all), c(1, 3, 4, 7, 8)
for (i in 1:2) {
initial_dataset()
start_time_ds <- Sys.time()
## create lexicon for candidate search
# ground_truth lexicon
current_gt <- paste(gt_lines_all[[i]], collapse = " ")
current_gt_vec <- str_split(current_gt, " ")[[1]]
additional_file <- sample((1:100)[-i],  1)
all_gt <- paste(c(gt_lines_all[[i]],
gt_lines_all[[additional_file[1]]]), collapse = " ")
all_gt_vec <- str_split(all_gt," ")[[1]]
all_gt_vec <- unique(all_gt_vec)
## search error_term and corresponding ground truth for each file
error_position <- which(MatchDetect[[i]]$Detect == FALSE)
error_term <- as.character(MatchDetect[[i]]$Tesseract[error_position])
gt_term <- as.character(MatchDetect[[i]]$GroundTruth[error_position])
## for each error  #length(error_term)
for (j in 1:2){
w_e <- error_term[j] # word_error
## create a candidate set for each error
leven_distance <- sapply(all_gt_vec, stringdist, b = w_e, method = "lv") # calculate Levenshtein distance between error term and each candidate in all_gt_vec
chosen_candidate <- order(leven_distance)[1:s] # chose first s candidates position
candidate_set <- all_gt_vec[chosen_candidate]
## feature scoring
### feature1: Levenshtein edit distance
# Levenshtein distance for the candidate
dist_candidate <- as.numeric(leven_distance[chosen_candidate])
score_led <- 1 - dist_candidate/(delta + 1)
### feature2: String similarity
score_ss <- string_similiarity(w_e, candidate_set)
### feature3: Language popularity
frequency3 <- unlist(lapply(candidate_set, freq5, unigram))
score_lp <- context_score(frequency3)
### feature4: lexicon existance
score_le <- as.numeric(candidate_set%in%current_gt_vec)
### feature5 and 6: exact-context popularity andrelaxed-context popularity
# neighboring terms
minus2 <-  as.character(MatchDetect[[i]]$Tesseract[error_position[j] - 2])
minus1 <- as.character(MatchDetect[[i]]$Tesseract[error_position[j] - 1])
positive1 <-  as.character(MatchDetect[[i]]$Tesseract[error_position[j] + 1])
positive2 <-  as.character(MatchDetect[[i]]$Tesseract[error_position[j] + 2])
frequency5 <- frequency6 <- NULL
## feature5 and feature6
# for each candidate, calculate ngram frequency
for (k in 1:length(candidate_set)) {
# tested candidate
candidate <- candidate_set[k]
# feature5: exact-context popularity
ngram1 <- paste(c(minus2, minus1, candidate), collapse = " ")
ngram2 <- paste(c(minus1, candidate, positive1), collapse = " ")
ngram3 <- paste(c(candidate, positive1, positive2), collapse = " ")
# candidate ngram freq (exact)
candidate_frequency <- sum(freq5(ngram1, three_gram),
freq5(ngram2, three_gram),
freq5(ngram3, three_gram))
frequency5 <- append(frequency5, candidate_frequency)
# feature6: relaxed-context popularity
ngram1_1 <- paste(c(".", minus1, candidate), collapse = " ")
ngram1_2 <- paste(c(minus2, ".", candidate), collapse = " ")
ngram2_1 <- paste(c(".", candidate, positive1), collapse = " ")
ngram2_2 <- paste(c(minus1, candidate, "."), collapse = " ")
ngram3_1 <- paste(c(candidate, ".", positive2), collapse = " ")
ngram3_2 <- paste(c(candidate, positive1, "."), collapse = " ")
# candidate ngram freq (relaxed)
candidate_frequency <- sum(freq6(ngram1_1, three_gram),
freq6(ngram1_2, three_gram),
freq6(ngram2_1, three_gram),
freq6(ngram2_2, three_gram),
freq6(ngram3_1, three_gram),
freq6(ngram3_2, three_gram))
frequency6 <- append(frequency6, candidate_frequency)
}
score_ecp <- context_score(frequency5)
score_rcp <- context_score(frequency6)
data_set <- add_row(data_set, "error_term" = rep(w_e, length(candidate_set)),
"gt_term" = rep(gt_term[j], length(candidate_set)),
"candidate" = candidate_set,
"feature1" = score_led,
"feature2" = score_ss,
"feature4" = score_le,
"feature3" = score_lp,
"feature5" = score_ecp,
"feature6" = score_rcp,
"file_index" = rep(i, length(candidate_set)),
"error_index" = rep(error_position[j], length(candidate_set)))
}
# the loop for each file
data_set$y <- as.numeric(data_set$gt_term == data_set$candidate)
end_time_ds <- Sys.time()
# time_ds <- end_time_ds - start_time_ds ## file1, 3, 4, 7, 8; first 20 errors for each file; first 5 candidates for each error
print(paste("Running time for building dataset = ", (end_time_ds - start_time_ds)))
filename = paste("dataset", i, sep = "_")
save(data_set, file=paste(filepath, filename))
}
#
# data_set$y <- as.numeric(data_set$gt_term == data_set$candidate)
# end_time_ds <- Sys.time()
# time_ds <- end_time_ds - start_time_ds ## file1, 3, 4, 7, 8; first 20 errors for each file; first 5 candidates for each error
# print(paste("Running time for building dataset = ", time_ds))
# filepath = "../output/"
i = 1
paste(paste("dataset", i, sep = "_"), "RData", sep = ".")
same_na = rep(NA, n_file)
for(i in 1:n_file){
## i represents that this is the i-th file we are dealing with
current_file_name <- sub(".txt","",file_name_vec[i])
## read the ground truth text
current_ground_truth_txt = readLines(paste("../data/ground_truth_trimmed/",current_file_name,".txt",sep=""), warn=FALSE)
## read the tesseract text
current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)
## only keep the line pairs that have the same number of words
n_line = length(current_ground_truth_txt)
for (j in 1:n_line){
same_len_logi = length(str_split(current_ground_truth_txt[j], " ")[[1]]) ==
length(str_split(current_tesseract_txt[j], " ")[[1]])
if(!same_len_logi){
current_ground_truth_txt[j] = NA
current_tesseract_txt[j] = NA
}
}
na_logi1 = is.na(current_ground_truth_txt)
na_logi2 = is.na(current_tesseract_txt)
same_na[i] = all(na_logi1 == na_logi2) # check if all the lines are assigned accordingly
## save and vectorized cleaned text
gt_lines_all[[i]] = current_ground_truth_txt[!na_logi1]
gt_vec_all[[i]] <- str_split(paste(gt_lines_all[[i]], collapse = " ")," ")[[1]]
tesseract_lines_all[[i]] = current_tesseract_txt[!na_logi2]
tesseract_vec_all[[i]] = str_split(paste(tesseract_lines_all[[i]], collapse = " ")," ")[[1]]
## re-check if each tesseract file has the same number of length with its corresponding ground truth
len_check[i] = length(tesseract_lines_all[[i]]) == length(gt_lines_all[[i]])
}
same_na = rep(NA, n_file)
for(i in 1:n_file){
## i represents that this is the i-th file we are dealing with
current_file_name <- sub(".txt","",file_name_vec[i])
## read the ground truth text
current_ground_truth_txt = readLines(paste("../data/ground_truth_trimmed/",current_file_name,".txt",sep=""), warn=FALSE)
## read the tesseract text
current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)
## only keep the line pairs that have the same number of words
n_line = length(current_ground_truth_txt)
for (j in 1:n_line){
same_len_logi = length(str_split(current_ground_truth_txt[j], " ")[[1]]) ==
length(str_split(current_tesseract_txt[j], " ")[[1]])
if(!same_len_logi){
current_ground_truth_txt[j] = NA
current_tesseract_txt[j] = NA
}
}
na_logi1 = is.na(current_ground_truth_txt)
na_logi2 = is.na(current_tesseract_txt)
same_na[i] = all(na_logi1 == na_logi2) # check if all the lines are assigned accordingly
## save and vectorized cleaned text
gt_lines_all[[i]] = current_ground_truth_txt[!na_logi1]
gt_vec_all[[i]] <- str_split(paste(gt_lines_all[[i]], collapse = " ")," ")[[1]]
tesseract_lines_all[[i]] = current_tesseract_txt[!na_logi2]
tesseract_vec_all[[i]] = str_split(paste(tesseract_lines_all[[i]], collapse = " ")," ")[[1]]
## re-check if each tesseract file has the same number of length with its corresponding ground truth
len_check[i] = length(tesseract_lines_all[[i]]) == length(gt_lines_all[[i]])
}
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
# read ground truth text into 1 list
n_file = length(file_name_vec)
tesseract_vec_all = list() # each element is a vector of words for each text file
tesseract_lines_all = list() # each element is a list of lines for each text file
gt_vec_all = list() # each element is a vector of words for each text file
gt_lines_all = list() # each element is a list of lines for each text file
len_check = rep(NA, 100)
len_check2 = rep(NA, 100)
## check if each tesseract file has the same number of length with its corresponding ground truth
for(i in 1:n_file){
## i represents that this is the i-th file we are dealing with
current_file_name <- sub(".txt","",file_name_vec[i])
## read the ground truth text, save it by lines & vectorize it
current_ground_truth_txt = readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
## read the tesseract text, save it by lines & vectorize it
current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)
## check if each tesseract file has the same number of length with its corresponding ground truth
len_check[i] = length(current_ground_truth_txt) == length(current_tesseract_txt)
## check if all ground truth are not shorter than its Tesseract
len_check2[i] = length(current_ground_truth_txt) >= length(current_tesseract_txt)
}
# read ground truth text into 1 list
n_file = length(file_name_vec)
tesseract_vec_all = list() # each element is a vector of words for each text file
tesseract_lines_all = list() # each element is a list of lines for each text file
gt_vec_all = list() # each element is a vector of words for each text file
gt_lines_all = list() # each element is a list of lines for each text file
len_check = rep(NA, 100)
len_check2 = rep(NA, 100)
## check if each tesseract file has the same number of length with its corresponding ground truth
for(i in 1:n_file){
## i represents that this is the i-th file we are dealing with
current_file_name <- sub(".txt","",file_name_vec[i])
## read the ground truth text, save it by lines & vectorize it
current_ground_truth_txt = readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
## read the tesseract text, save it by lines & vectorize it
current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)
## check if each tesseract file has the same number of length with its corresponding ground truth
len_check[i] = length(current_ground_truth_txt) == length(current_tesseract_txt)
## check if all ground truth are not shorter than its Tesseract
len_check2[i] = length(current_ground_truth_txt) >= length(current_tesseract_txt)
}
setwd("~/Documents/Github/ADS/Fall2018-Project4-sec2--sec2proj4_grp8/doc")
# read ground truth text into 1 list
n_file = length(file_name_vec)
tesseract_vec_all = list() # each element is a vector of words for each text file
tesseract_lines_all = list() # each element is a list of lines for each text file
gt_vec_all = list() # each element is a vector of words for each text file
gt_lines_all = list() # each element is a list of lines for each text file
len_check = rep(NA, 100)
len_check2 = rep(NA, 100)
## check if each tesseract file has the same number of length with its corresponding ground truth
for(i in 1:n_file){
## i represents that this is the i-th file we are dealing with
current_file_name <- sub(".txt","",file_name_vec[i])
## read the ground truth text, save it by lines & vectorize it
current_ground_truth_txt = readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
## read the tesseract text, save it by lines & vectorize it
current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)
## check if each tesseract file has the same number of length with its corresponding ground truth
len_check[i] = length(current_ground_truth_txt) == length(current_tesseract_txt)
## check if all ground truth are not shorter than its Tesseract
len_check2[i] = length(current_ground_truth_txt) >= length(current_tesseract_txt)
}
